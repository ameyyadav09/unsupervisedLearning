{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trip to the grocery store provides many examples of machine learning in action today and future uses of it. The way items are displayed, the coupons offered to you after you purchase something, and loyalty programs all are driven by massive amounts of data crunching.\n",
    "\n",
    "Loyalty programs, used to give the customers a discount, can provide the store what a customer is purchasing. In case if the customer did not use loyalty card they will look at his/her credit card. If the payment mode is cash then store looks at the type of products that are bought together by that customer.\n",
    "\n",
    "And looking at the common items that are frequently bought together gives store the behavioral patterns of the customer. And finding the hidden relationships in the data is called as association analysis. And finding those kind of patterns is a very time consuming and computationally expensive task. \n",
    "\n",
    "Apriori algorithm is used to perform association rule mining, that in turn are used to extract frequent items sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Association analysis\n",
    "\n",
    "Association analysis is the task of finding interesting relationships in large datasets. These relationships can take two forms\n",
    "- frequent item sets: Collection of items that frequently occur together\n",
    "- Association rules: These rules suggest that a strong relationship exists between two items. \n",
    "\n",
    "Example:\n",
    "A list of transactions from a grocery store is shown in the below table.\n",
    " <table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Transaction number</th>\n",
    "    <th>Items</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>soy milk, lettuce</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>lettuce, diapers, wine, curd</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>soy milk, diapers, wine, orange juice</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>lettuce, soy milk, wine, diapers</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>lettuce, diapers, soy milk, orange juice</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "From this example, in transaction 1 {soy milk, lettuce} were bought together now they are called as a frequent item set. \n",
    "And from this data we can find an association rule diapers -> wine. Which means if someone buys diapers then there is very high probability for them to buy wine. By identifying these kind of frequent item sets and association rules sellers can understand the customers behavious much more.\n",
    "\n",
    "Another best example of association analysis is <a href=\"http://www.dssresources.com/newsletters/66.php\">here</a>.\n",
    "In this a store in the midwest US noticed that men bought diapers and beer frequently on thrusdays. Now all the store has to do is place those both items nearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But,\n",
    "- How do we define these so-called interesting relationships.\n",
    "- Who defines what's interesting\n",
    "- While looking for frequent sets, what's the definition of frequent?\n",
    "    \n",
    "Like these there could be innumerable questions that need to be answered, many concepts are used to explain above questions but two most important are support and confidence.\n",
    "\n",
    "**Support** of an itemset is defined as the percentage of the dataset that contains this itemset.\n",
    "\n",
    "Ex: From above image \n",
    "\n",
    "**Support**({soy milk}) = 4/5\n",
    "\n",
    "**Support**({soy milk, diaper}) = 3/5\n",
    "\n",
    "Support is applied to an itemset, hence by specifying a threshould value we can select itemsets that meet our criteria.\n",
    "\n",
    "\n",
    "**Confidence** is defined for an association rule like , **{diapers} -> {wine}**.<br>\n",
    "The confidence for this rule is defined as **support**({diapers, wine})/**support**({diapers}).\n",
    "\n",
    "Ex: **Support**({diapers, wine}) = 3/5\n",
    "\n",
    "**Support**({diapers}) = 4/5\n",
    "\n",
    "**Confidence**({diapers} -> {wine}) = 3/4\n",
    "\n",
    "The support and confidence are ways we can quantify the success of our association analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Apriori Principle\n",
    "Let’s assume that we are running a grocery store with a very limited selection. We are interested in finding out which items were purchased together. \n",
    "\n",
    "We have only four items: item0, item1, item2, and item3. What are all the possible combinations in which they can be purchased? We can have one item, say item0, alone, or two items, or three items, or all of the items together. \n",
    "\n",
    "**Note** If someone purchased two of item0 and four of item2, we don’t care. We’re concerned only that they purchased one or more of an item.\n",
    "\n",
    "General approach to the Apriori algorithm\n",
    "\n",
    "    1.Collect: Any method.\n",
    "    2.Prepare: Any data type will work as we’re storing sets.\n",
    "    3.Analyze: Any method.\n",
    "    4.Train: Use the Apriori algorithm to find frequent itemsets.\n",
    "    5.Test: Doesn’t apply.\n",
    "    6.Use: This will be used to find frequent itemsets and association rules between items.\n",
    "    \n",
    "<img src=\"images/apriori.png\" title=\"All possible sets from available 4 images\">\n",
    "\n",
    "Above diagram shows all possible combinations of the items. big Ø means null set or a set containing no items. Lines connecting item sets indicate that two or more sets can be combined to form a larger set.\n",
    "\n",
    "Remember that our goal is to find sets of items that are purchased together frequently. We measured frequency by the support of a set. The support of a set counted the percentage of transactions that contained that set. If we need to caluculate support for a set say {0,3}? we go through every transaction and ask, ”Did this transaction contain 0 and 3?” If the transaction did contain both those items, we increment the total. After scanning all of our data, we divide the total by the number of transactions, and we have our support. This result is for only one set: {0,3}. Like this we have to repeat this to get the support for every possible set. We can count the sets in above figure and see that for four items, we have to go over the data 15 times. This number gets large quickly. A data set that contains N possible items can generate 2N-1 possible itemsets. Stores selling 10,000 or more items aren’t uncommon. Even a store selling 100 items can generate $$1.26*10^{30} possible \\space itemsets $$ \n",
    "\n",
    "This would take a very long time to compute on a modern computer.\n",
    "\n",
    "To reduce the time needed to compute this value, Apriori principle was devised. The Apriori principle helps us reduce the number of possible interesting itemsets. The Apriori principle says that if an itemset is frequent,\n",
    "then all of its subsets are frequent. In above image this means that if {0,1} is frequent, then {0} and {1} have to be frequent. This rule as it is doesn’t help us, but if we turn it inside out? that if an itemset is infrequent, then its supersets are also infrequent.\n",
    "\n",
    "<img src=\"images/infreq.png\" title=\"showing infrequent items\">\n",
    "\n",
    "\n",
    "In above image, the shaded itemset {2,3} is considered to be infrequent. From this information, we know that itemsets {0,2,3}, {1,2,3}, and {0,1,2,3} are also infrequent. This tells us that once we’ve computed the support of {2,3}, we don’t have to compute the support of {0,2,3}, {1,2,3}, and {0,1,2,3} because we know they won’t meet our requirements. Using this principle, we can halt the exponential growth of itemsets and compute a list of frequent item sets in a comparatively less time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finding frequent itemsets with Apriori Algorithm\n",
    "The Apriori algorithm needs a minimum support level as an input and a data set. The algorithm will generate a list of all candidate itemsets with one item. The transaction data set will then be scanned to see which sets meet the minimum support level. Sets that don’t meet the minimum support level will get tossed out. The remaining sets will then be combined to make itemsets with two elements. Again, the transaction dataset will be scanned and itemsets not meeting the minimum support level will get tossed. This procedure will be repeated until all sets are tossed out.\n",
    "\n",
    "**Psuedo Code**\n",
    "    - While the number of items in the set is greater than 0:\n",
    "        - Create a list of candidate itemsets of length k\n",
    "        - Scan the dataset to see if each itemset is frequent\n",
    "        - Keep frequent itemsets to create itemsets of length k+1\n",
    "\n",
    "<img src=\"images/data_full.png\">\n",
    "\n",
    "The first step of Apriori is to count up the frequencies, called the supports, of each member item separately.\n",
    "\n",
    "<img src=\"images/data_freqs.png\">\n",
    "\n",
    "We can define a minimum support level to qualify as \"frequent,\" which depends on the context. For this case, let min support = 4. Therefore, all are frequent. The next step is to generate a list of all 2-pairs of the frequent items. Had any of the above items not been frequent, they wouldn't have been included as a possible member of 2-item  pairs.\n",
    "\n",
    "In next step we again select only 2-pairs item which are frequent.\n",
    "\n",
    "<img src=\"images/freq_sets.png\">\n",
    "\n",
    "Next we generate the list of all 3-triples of the frequent items(by connecting frequent pair with frequent single item)\n",
    "\n",
    "<img src=\"images/freq_sets3.png\">\n",
    "\n",
    "The algorithm will end here because the pair{2,3,4,5} generated at the next step does not have the desired support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mining association rules from frequent item sets\n",
    "\n",
    "To find association rules, we first start with a frequent itemset. We know this set of items is unique, but we want to see if there is anything else we can get out of these items. **One item or one set of items can imply another item.**\n",
    "\n",
    "From the grocery store example, if we have a frequent itemset, {soy milk, lettuce}, one example of an  association rule is soy milk ➞ lettuce. This means if someone purchases soy milk, then there’s a statistically significant chance that they’ll purchase lettuce. The converse doesn’t always hold. That is, just because {soy milk} ➞ {lettuce} is statistically significant, it doesn’t mean that {lettuce} ➞ {soy milk} is statistically significant.\n",
    "\n",
    "we quantified an itemset as frequent if it met our minimum support level. We have a similar measurement for  association rules called the **confidence**.\n",
    "\n",
    "**confidence(P ➞ H)  = support({P U H})/ support({P})**\n",
    "\n",
    "we can generate many association rules for each frequent itemset. It would be good if we could reduce the number of rules to keep the problem tractable. We can observe that if a rule doesn’t meet the minimum confidence requirement, then subsets of that rule also won’t meet the minimum.\n",
    "\n",
    "<img src=\"images/association.png\">\n",
    "Assume that the rule 0,1,2 ➞ 3 doesn’t meet the minimum confidence. We know that any rule where the left-hand side is a subset of {0,1,2} will also not meet the minimum confidence.\n",
    "\n",
    "We can use this property of association rules to reduce the number of rules we need to test. Similar to the Apriori algorithm, we can start with a frequent itemset. We’ll then create a list of sets with one item on the right-hand side and test all of those. Next, we’ll merge the remaining rules to create a list of rules with two\n",
    "items on the right-hand side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example: finding similar features in poisonous mushrooms\n",
    "\n",
    "In this example, we are going to look for common features in poisonous mushrooms. You can then use these common features to avoid eating poisonous mushrooms.\n",
    "\n",
    "The <a href=\"http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\">UCI Machine Learning Repository</a> has a dataset with 23 features taken from species of gilled mushrooms. Each of the features contains nominal values. We are going to need to transform these nominal values into a set of transaction data.\n",
    "\n",
    "Luckily, this transformation was already done for us <a href=\"http://fimi.ua.ac.be/data/\">here</a>\n",
    "\n",
    "Roberto Bayardo has parsed the UCI mushrooms dataset into a set of features for each sample of mushroom. Each possible value for each feature is enumerated, and if a sample contains that feature, then its integer value is  included in the dataset.\n",
    "\n",
    "The transformed dataset is found <a href=\"http://fimi.ua.ac.be/data/mushroom.dat\">here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9  ...  14  15  16  17  18  19  20  \\\n",
      "0   1   3   9  13  23  25  34  36  38  40 ...  67  76  85  86  90  93  98   \n",
      "1   2   3   9  14  23  26  34  36  39  40 ...  67  76  85  86  90  93  99   \n",
      "2   2   4   9  15  23  27  34  36  39  41 ...  67  76  85  86  90  93  99   \n",
      "3   1   3  10  15  23  25  34  36  38  41 ...  67  76  85  86  90  93  98   \n",
      "4   2   3   9  16  24  28  34  37  39  40 ...  67  76  85  86  90  94  99   \n",
      "\n",
      "    21   22  23  \n",
      "0  107  113 NaN  \n",
      "1  108  114 NaN  \n",
      "2  108  115 NaN  \n",
      "3  107  113 NaN  \n",
      "4  109  114 NaN  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "import pandas as pd\n",
    "mushDatSet = pd.read_csv(\"http://fimi.ua.ac.be/data/mushroom.dat\", header = None, delimiter = ' ')\n",
    "print(mushDatSet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n"
     ]
    }
   ],
   "source": [
    "#searching for null values \n",
    "not_nulls = mushDatSet.columns[mushDatSet.notna().any()].tolist()\n",
    "print(not_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 23)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping null values\n",
    "mushDatSet = mushDatSet.iloc[:, not_nulls]\n",
    "mushDatSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first feature is poisonous or edible. If a sample is poisonous, you get a 1. If it’s edible, you get a 2. The next feature is cap shape, which has six possible values that are represented with the integers 3–8. To find features in common with poisonous mushrooms, you can run the Apriori algorithm and look for itemsets with feature2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidate(dataSet):\n",
    "    C1 = []\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if not [item] in C1:\n",
    "                C1.append([item])\n",
    "                C1 = sorted(C1)\n",
    "    return list(map(frozenset, C1))\n",
    "\n",
    "def scanD(D, Ck, minSupport):\n",
    "    ssCnt = {}\n",
    "    for tid in D:\n",
    "        for can in Ck:\n",
    "            if can.issubset(tid):\n",
    "                if can not in ssCnt: \n",
    "                    ssCnt[can]=1\n",
    "                else: \n",
    "                    ssCnt[can] += 1\n",
    "    numItems = float(len(D))\n",
    "    retList = []\n",
    "    supportData = {}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key]/numItems\n",
    "        if support >= minSupport:\n",
    "            retList.insert(0,key)\n",
    "        supportData[key] = support\n",
    "    return retList, supportData\n",
    "\n",
    "def aprioriGen(Lk, k):\n",
    "    retList = []\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i+1, lenLk):\n",
    "            L1 = list(Lk[i])[:k-2]; L2 = list(Lk[j])[:k-2]\n",
    "            L1.sort(); L2.sort()\n",
    "            if L1==L2:\n",
    "                retList.append(Lk[i] | Lk[j])\n",
    "    return retList\n",
    "\n",
    "def apriori(dataSet, minSupport = 0.5):\n",
    "    C1 = create_candidate(dataSet)\n",
    "    D = list(map(set, dataSet))\n",
    "    L1, supportData = scanD(D, C1, minSupport)\n",
    "    L = [L1]\n",
    "    k = 2\n",
    "    while (len(L[k-2]) > 0):\n",
    "        Ck = aprioriGen(L[k-2], k)\n",
    "        Lk, supK = scanD(D, Ck, minSupport)\n",
    "        supportData.update(supK)\n",
    "        L.append(Lk)\n",
    "        k += 1\n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,suppData=apriori(mushDatSet.values.tolist(), minSupport=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({2, 28})\n",
      "frozenset({2, 53})\n",
      "frozenset({2, 23})\n",
      "frozenset({2, 34})\n",
      "frozenset({2, 36})\n",
      "frozenset({2, 59})\n",
      "frozenset({2, 63})\n",
      "frozenset({2, 67})\n",
      "frozenset({2, 76})\n",
      "frozenset({2, 85})\n",
      "frozenset({2, 86})\n",
      "frozenset({2, 90})\n",
      "frozenset({2, 93})\n",
      "frozenset({2, 39})\n"
     ]
    }
   ],
   "source": [
    "# Now you can search the frequent itemsets for the poisonous feature 2\n",
    "for item in L[1]:\n",
    "    if item.intersection([2]):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({2, 34, 53, 28})\n",
      "frozenset({2, 34, 59, 28})\n",
      "frozenset({2, 59, 28, 63})\n",
      "frozenset({2, 85, 59, 28})\n",
      "frozenset({2, 86, 59, 28})\n",
      "frozenset({2, 90, 59, 28})\n",
      "frozenset({2, 39, 59, 28})\n",
      "frozenset({2, 34, 28, 63})\n",
      "frozenset({2, 85, 53, 28})\n",
      "frozenset({2, 34, 85, 28})\n",
      "frozenset({2, 85, 28, 63})\n",
      "frozenset({2, 53, 86, 28})\n",
      "frozenset({2, 34, 86, 28})\n",
      "frozenset({2, 86, 28, 63})\n",
      "frozenset({2, 85, 86, 28})\n",
      "frozenset({2, 53, 90, 28})\n",
      "frozenset({2, 34, 90, 28})\n",
      "frozenset({2, 85, 90, 28})\n",
      "frozenset({2, 86, 90, 28})\n",
      "frozenset({2, 53, 39, 28})\n",
      "frozenset({2, 34, 39, 28})\n",
      "frozenset({2, 39, 28, 63})\n",
      "frozenset({2, 85, 39, 28})\n",
      "frozenset({2, 86, 39, 28})\n",
      "frozenset({2, 39, 90, 28})\n",
      "frozenset({2, 34, 53, 85})\n",
      "frozenset({2, 34, 53, 86})\n",
      "frozenset({2, 34, 53, 90})\n",
      "frozenset({2, 34, 53, 39})\n",
      "frozenset({2, 85, 53, 86})\n",
      "frozenset({2, 85, 53, 39})\n",
      "frozenset({2, 53, 85, 90})\n",
      "frozenset({2, 53, 86, 90})\n",
      "frozenset({2, 53, 39, 90})\n",
      "frozenset({2, 53, 86, 39})\n",
      "frozenset({2, 34, 36, 23})\n",
      "frozenset({2, 34, 23, 59})\n",
      "frozenset({2, 34, 23, 63})\n",
      "frozenset({2, 34, 36, 63})\n",
      "frozenset({2, 34, 85, 23})\n",
      "frozenset({2, 34, 36, 85})\n",
      "frozenset({2, 34, 85, 63})\n",
      "frozenset({2, 34, 85, 76})\n",
      "frozenset({2, 34, 86, 23})\n",
      "frozenset({2, 34, 36, 86})\n",
      "frozenset({2, 34, 86, 63})\n",
      "frozenset({2, 34, 86, 76})\n",
      "frozenset({2, 34, 85, 86})\n",
      "frozenset({2, 34, 23, 90})\n",
      "frozenset({2, 34, 36, 90})\n",
      "frozenset({2, 34, 90, 63})\n",
      "frozenset({2, 34, 85, 90})\n",
      "frozenset({2, 34, 86, 90})\n",
      "frozenset({2, 34, 23, 93})\n",
      "frozenset({2, 34, 36, 93})\n",
      "frozenset({2, 34, 93, 63})\n",
      "frozenset({2, 34, 85, 93})\n",
      "frozenset({2, 34, 86, 93})\n",
      "frozenset({2, 34, 90, 93})\n",
      "frozenset({2, 34, 23, 39})\n",
      "frozenset({2, 34, 36, 39})\n",
      "frozenset({2, 34, 39, 63})\n",
      "frozenset({2, 34, 39, 76})\n",
      "frozenset({2, 34, 85, 39})\n",
      "frozenset({2, 34, 86, 39})\n",
      "frozenset({2, 34, 39, 90})\n",
      "frozenset({2, 34, 39, 93})\n",
      "frozenset({2, 36, 23, 59})\n",
      "frozenset({2, 34, 36, 59})\n",
      "frozenset({2, 36, 23, 63})\n",
      "frozenset({2, 36, 85, 23})\n",
      "frozenset({2, 36, 85, 63})\n",
      "frozenset({2, 36, 86, 23})\n",
      "frozenset({2, 36, 86, 63})\n",
      "frozenset({2, 36, 85, 86})\n",
      "frozenset({2, 36, 90, 63})\n",
      "frozenset({2, 36, 85, 90})\n",
      "frozenset({2, 36, 86, 90})\n",
      "frozenset({2, 36, 23, 93})\n",
      "frozenset({2, 36, 93, 63})\n",
      "frozenset({2, 36, 85, 93})\n",
      "frozenset({2, 36, 86, 93})\n",
      "frozenset({2, 36, 90, 93})\n",
      "frozenset({2, 36, 23, 39})\n",
      "frozenset({2, 36, 39, 63})\n",
      "frozenset({2, 36, 85, 39})\n",
      "frozenset({2, 36, 86, 39})\n",
      "frozenset({2, 36, 39, 90})\n",
      "frozenset({2, 36, 39, 93})\n",
      "frozenset({2, 23, 59, 63})\n",
      "frozenset({2, 34, 59, 63})\n",
      "frozenset({2, 36, 59, 63})\n",
      "frozenset({2, 85, 23, 59})\n",
      "frozenset({2, 34, 85, 59})\n",
      "frozenset({2, 36, 85, 59})\n",
      "frozenset({2, 85, 59, 63})\n",
      "frozenset({2, 86, 23, 59})\n",
      "frozenset({2, 34, 86, 59})\n",
      "frozenset({2, 36, 86, 59})\n",
      "frozenset({2, 86, 59, 63})\n",
      "frozenset({2, 85, 86, 59})\n",
      "frozenset({2, 23, 90, 59})\n",
      "frozenset({2, 34, 90, 59})\n",
      "frozenset({2, 36, 90, 59})\n",
      "frozenset({2, 90, 59, 63})\n",
      "frozenset({2, 85, 90, 59})\n",
      "frozenset({2, 86, 90, 59})\n",
      "frozenset({2, 23, 59, 93})\n",
      "frozenset({2, 34, 59, 93})\n",
      "frozenset({2, 36, 59, 93})\n",
      "frozenset({2, 59, 93, 63})\n",
      "frozenset({2, 85, 59, 93})\n",
      "frozenset({2, 86, 59, 93})\n",
      "frozenset({2, 90, 59, 93})\n",
      "frozenset({2, 23, 39, 59})\n",
      "frozenset({2, 34, 39, 59})\n",
      "frozenset({2, 36, 39, 59})\n",
      "frozenset({2, 39, 59, 63})\n",
      "frozenset({2, 85, 39, 59})\n",
      "frozenset({2, 86, 39, 59})\n",
      "frozenset({2, 39, 90, 59})\n",
      "frozenset({2, 39, 59, 93})\n",
      "frozenset({2, 85, 23, 63})\n",
      "frozenset({2, 86, 23, 63})\n",
      "frozenset({2, 23, 39, 63})\n",
      "frozenset({2, 67, 34, 85})\n",
      "frozenset({2, 67, 34, 86})\n",
      "frozenset({2, 67, 85, 86})\n",
      "frozenset({2, 67, 34, 39})\n",
      "frozenset({2, 67, 85, 39})\n",
      "frozenset({2, 67, 86, 39})\n",
      "frozenset({2, 85, 86, 76})\n",
      "frozenset({2, 85, 39, 76})\n",
      "frozenset({2, 86, 39, 76})\n",
      "frozenset({2, 85, 86, 23})\n",
      "frozenset({2, 85, 86, 63})\n",
      "frozenset({2, 85, 23, 90})\n",
      "frozenset({2, 85, 90, 63})\n",
      "frozenset({2, 85, 23, 93})\n",
      "frozenset({2, 85, 93, 63})\n",
      "frozenset({2, 23, 85, 39})\n",
      "frozenset({2, 85, 39, 63})\n",
      "frozenset({2, 85, 86, 39})\n",
      "frozenset({2, 86, 23, 90})\n",
      "frozenset({2, 86, 90, 63})\n",
      "frozenset({2, 85, 86, 90})\n",
      "frozenset({2, 86, 23, 93})\n",
      "frozenset({2, 86, 93, 63})\n",
      "frozenset({2, 85, 86, 93})\n",
      "frozenset({2, 23, 86, 39})\n",
      "frozenset({2, 86, 39, 63})\n",
      "frozenset({2, 23, 90, 93})\n",
      "frozenset({2, 90, 93, 63})\n",
      "frozenset({2, 85, 90, 93})\n",
      "frozenset({2, 86, 90, 93})\n",
      "frozenset({2, 39, 90, 63})\n",
      "frozenset({2, 85, 39, 90})\n",
      "frozenset({2, 86, 39, 90})\n",
      "frozenset({2, 23, 39, 93})\n",
      "frozenset({2, 39, 93, 63})\n",
      "frozenset({2, 85, 39, 93})\n",
      "frozenset({2, 86, 39, 93})\n",
      "frozenset({2, 39, 90, 93})\n"
     ]
    }
   ],
   "source": [
    "# You can also repeat this for the larger itemsets\n",
    "for item in L[3]:\n",
    "    if item.intersection([2]):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to look up these features so you know what to look for in wild mushrooms. If you see any of these features, avoid eating the mushroom. Although these features may be common in poisonous mushrooms, the absence of these features doesn’t make a mushroom edible. Eating the wrong mushroom will still kill a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
