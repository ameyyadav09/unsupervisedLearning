{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Curse of Dimensionality\n",
    "We are so used to living in 3 dimensions that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (below image)\n",
    "\n",
    "<img src=\"images/curseOfDims.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that many things behave very differently in high-dimensional space and as the dimensions in our data set increase, the number of samples required for an estimator to generalize also increase exponentially. And so problems involving high dimensional data become victims of **Curse of Dimensionality**\n",
    "\n",
    "But it's not always feasible to to acquire such large data sets and at the same time learning from those data sets requires more memory and processing power.\n",
    "\n",
    "Eventually the sparseness of the data also increases with its dimensions. With this it becomes even harder for the estimator to detect similar instances in a higher dimensional space as all of the data points are similarly sparse.\n",
    "\n",
    "**Example**: If you pick two points randomly in a **unit square**, the distance between these two points will be, on average, roughly **0.52**. If you pick two random points in a **unit 3D cube**, the average distance will be roughly **0.66**. But what about two points picked randomly in a **one-million-dimensional hypercube**? Well the average distance, will be about **408.25**\n",
    "\n",
    "$$(roughly \\sqrt{1, 000, 000/6})! $$\n",
    "\n",
    "This is quite counter-intuitive, how can two points be so far apart when they both lie within the same unit hypercube? This fact implies that highdimensional datasets are at risk of being very sparse, most training data points are likely to be very far away from each other. Of course this also means that a new instance will also be far away from training set, making predictions much less promising than in lower dimensional space, as they will be exploited by the curse of dimensionality.\n",
    "\n",
    "Since increasing number of records is not possible in every case, we try to reduce the number of dimensions in the data to simplify our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:Reducing dimensionality does lose some information (just like compressing an image to JPEG can degrade its quality) so even though it will speed up training, it may also make your system perform slightly worse. It also makes your pipelines a bit more complex and thus harder to maintain. So you should first try to train your system with the original data before considering using reducing dimensionality if training is too slow. In some cases however,\n",
    "reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance (but in general it wonâ€™t, it will just speed up training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two main approaches for dimensionality reduction:\n",
    "- projection\n",
    "- manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Projection**\n",
    "In majority of the use cases data points are not spread out uniformly across the entire feature space. Most of them are constant or are highly correlated. Hence the data points lie within a much lower dimensional sub-space of the entire high dimensional space.\n",
    "<img src=\"images/projection1.png\" height=600 width=600>\n",
    "\n",
    "Here we can see that all the data points are lying close to plane(which is a 2D sub space of the 3D space). But if we project those same points on to a place of the axis, \n",
    "\n",
    "<img src=\"images/projection2.png\" height=600 width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**<br>\n",
    "Based on this intuition an approach named Principal Component Analysis (PCA) is devised. First it identifies the hyperplane that lies closest to the data, then it projects the data onto it. PCA rotates the data in such a way that the features become statistically uncorrelated inorder to make the data align with its principal components to maximize the variance. \n",
    "\n",
    "And this rotation is often followed by selecting only a subset of the newly found features on the basis of how highly those features are correlated with the target variable. The newly derived lower-dimensional data will preserve as much of the variance of the original data as possible.\n",
    "\n",
    "In general, an n-dimensional dataset can be reduced by projecting the dataset onto a k-dimensional subspace, where k is less than n. More formally, PCA can be used to find a set of vectors that span a subspace, which minimizes the sum of the squared errors of the projected data. This projection will retain the greatest proportion of the original data set's variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's assume that we have a data set as follows,\n",
    "<img src=\"images/pca1.png\">\n",
    "\n",
    "The data points approximately form a long and thin ellipse as they move away from origin to the top right of the plot. To reduce the dimensions in the data, we have to project those points onto a line. \n",
    "\n",
    "Following image shows two lines to project the data. \n",
    "<img src=\"images/pca2.png\">\n",
    "\n",
    "Data varies more across the dashed line when compared to the dotted line. So, the dashed line becomes the first principal component. The second principal component must be orthogonal to the first one. In other terms the second principal component must be independent and appears perpendicular to the first principal component. And following image explains that,\n",
    "<img src=\"images/pca3.png\">\n",
    "\n",
    "If we have a 3D data, the data should be rotated with respect to one of the axes. So that we end up with a dimension that has zero variance and we can then discard that dimension.\n",
    "\n",
    "<img src=\"images/pca4.png\">\n",
    "\n",
    "PCA is most useful when the variance in a data set is distributed unevenly across the dimensions. Like a three-dimensional data set with a spherical convex hull. PCA cannot be used effectively with this data set because there is equal variance in each dimension; none of the dimensions can be discarded without losing a significant amount of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Variance, Covariance and Covariance Matrices\n",
    "**variance** is a measure of how a set of values are spread out. Variance is calculated as the average of the squared differences of the values and mean of the values.\n",
    "$$s^2 = \\frac{\\sum_{i=1}^n(X_i-{\\overline{X}})^2}{n-1}$$\n",
    "\n",
    "**Covariance** is a measure of how much two features change together. It is a measure of the linear relationship between two variables. Co-variance is similar to the correlation between two variables but unlike correlation the values of co-variance are not standardized values vary with data and can range from positive infinity to negative infinty, because of which co-variance cannot say the strength or degree of correlation. \n",
    "\n",
    "But co-variance can say the direction of correlation, positive co-variance implies positive correlation and negative covariacne includes negative correlation and if the covariance of two features is zero, the variables are uncorrelated. \n",
    "\n",
    "**Note:** Uncorrelated variables are not necessarily independent, as covariance is only a measure of linear dependence.\n",
    "\n",
    "$$Cov(X, Y) = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})(Y_i - \\overline{Y})}{n-1}$$\n",
    "\n",
    "If the covariance is nonzero, the sign indicates whether the variables are positively or negatively correlated. When two variables are positively correlated, one increases as the other increases. When variables are negatively correlated, one variable decreases relative to its mean as the other variable increases relative to its mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A covariance matrix describes the covariance values between each pair of features/variables/dimensions in a data set. The element **(i, j)** indicates the covariance of the **ith** and **jth** dimensions of the data. For example, a covariance matrix for a three-dimensional data is given by the following matrix,\n",
    "\n",
    "$$C = \\begin{bmatrix}cov(x_1, x_1) & cov(x_1, x_2) & cov(x_1, x_3) \\\\cov(x_2, x_1) & cov(x_2, x_2) & cov(x_2,x_3) \\\\ cov(x_3,x_1) & cov(x_3, x_2) & cov(x_3, x_3) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.125  0.075 -1.275]\n",
      "[[ 0.04916667  0.01416667  0.01916667]\n",
      " [ 0.01416667  0.00916667 -0.00583333]\n",
      " [ 0.01916667 -0.00583333  0.04916667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[2, 0, -1.4], [2.2, 0.2, -1.5], [2.4, 0.1, -1], [1.9, 0, -1.2]])\n",
    "print(X.mean(axis = 0))\n",
    "print (np.cov(X.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eigenvectors and Eigenvalues\n",
    "A vector is described by a direction and magnitude, or length. An eigenvector of a matrix is a non-zero vector that satisfies the following equation,\n",
    "\n",
    "$$A\\vec{v}=\\lambda\\vec{v} -------------------- (1)$$ \n",
    "\n",
    "In the preceding equation, A is a square matrix\n",
    "$$\\vec{v} \\space is \\space an \\space eigenvector$$\n",
    "$$\\lambda \\space is \\space a \\space scalar \\space called \\space an \\space eigenvalue.$$ \n",
    "\n",
    "The direction of an eigenvector remains the same after it has been transformed by A, only its magnitude changes, as indicated by the eigenvalue.\n",
    "\n",
    "That is, multiplying a matrix by one of its eigenvectors is equal to scaling the eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors and eigenvalues can only be derived from square matrices, and not all square matrices have eigenvectors or eigenvalues. If a matrix does have eigenvectors and eigenvalues, it will have a pair for each of its dimensions. \n",
    "\n",
    "**The principal components of a matrix are the eigenvectors of its covariance matrix**, ordered by their corresponding eigenvalues. The eigenvector with the greatest eigenvalue is the first principal component, the second principal component is the eigenvector with the second greatest eigenvalue, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "$$ A = \\begin{bmatrix}1 & -2 \\\\2 & -3 \\end{bmatrix}$$\n",
    "\n",
    "The product of A and any eigenvector of A must be equal to the eigenvector multiplied by its eigenvalue as per eqn(1). We will begin by finding the eigenvalues, which we can find using the following characteristic equations\n",
    "\n",
    "$$(A-\\lambda \\space I)\\vec{v}=0 ------------------ \\space(2)$$\n",
    "\n",
    "$$A-\\lambda I => \\begin{bmatrix}1 & -2 \\\\2 & -3 \\end{bmatrix} - \\begin{bmatrix}\\lambda & 0 \\\\0 & \\lambda\\end{bmatrix} = 0$$ \n",
    "\n",
    "As per the above equation the difference of the data matrix and the product of eigen value and Identity matrix equals zero.\n",
    "\n",
    "$$ \\begin{vmatrix}1-\\lambda & -2 \\\\2 & -3-\\lambda \\end{vmatrix} = (\\lambda+1)(\\lambda+1) = 0$$\n",
    "\n",
    "Both of the eigenvalues for this matrix are equal to -1. We can now use the eigenvalues to solve the eigenvectors using eqn(2)\n",
    "\n",
    "$$\\left(\\begin{bmatrix}1 & -2\\\\ 2 & -3\\end{bmatrix} - {\\begin{bmatrix}\\lambda & 0 \\\\0 & -\\lambda \\end{bmatrix} } \\right)\\vec{v} = {\\begin{bmatrix}1-\\lambda & -2 \\\\2 & -3-\\lambda \\end{bmatrix}\\vec{v} } = {\\begin{bmatrix}1-\\lambda & -2 \\\\2 & -3-\\lambda \\end{bmatrix} \\begin{bmatrix}v_{11} \\\\ v_{12}\\end{bmatrix} } $$\n",
    "\n",
    "We can now substitute eigen values and solve for eigenvectors.\n",
    "$$\\begin{bmatrix}1-(-1)&-2\\\\2&-3-(-1)\\end{bmatrix} \\begin{bmatrix}v_{11} \\\\ v_{12}\\end{bmatrix} = \\begin{bmatrix}2&-2 \\\\ 2&-2\\end{bmatrix}\\begin{bmatrix}v_{11} \\\\ v_{12}\\end{bmatrix} = 0$$\n",
    "\n",
    "Which transform to be, \n",
    "$$2v_{11}+-(2v_{1,2})=0$$\n",
    "$$2v_{11}+-(2v_{1,2})=0$$\n",
    "\n",
    "And any non-zero vector that satisfies the preceding equations, such as the following, can be used as an eigenvector\n",
    "\n",
    "$$\\begin{bmatrix}1&-2\\\\2&-3\\end{bmatrix}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix} = -1\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}  =\\begin{bmatrix}-1 \\\\ -1\\end{bmatrix}$$\n",
    "\n",
    "PCA requires unit eigenvectors, or eigenvectors that have a length equal to 1. We can normalize an eigenvector by dividing it by its norm, which is given by the following equation\n",
    "\n",
    "$$||x|| = \\sqrt{x_1^2 + x_2^2 + ... + x_n^2} --------------------- \\space (3)$$\n",
    "\n",
    "Hence from eqn(3) \n",
    "\n",
    "$$\\sqrt{1^2+1^2} = \\sqrt2 => \\begin{bmatrix}0.70710678118654\\\\0.70710678118654\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen Values [-0.99999998 -1.00000002]\n",
      "Eigen Vector [0.70710678 0.70710678]\n"
     ]
    }
   ],
   "source": [
    "# just to verify above results\n",
    "w, v = np.linalg.eig(np.array([[1, -2], [2, -3]]))\n",
    "print(\"Eigen Values\", w)\n",
    "print(\"Eigen Vector\",v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying PCA for higher dimensional data\n",
    "# importing dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "y = data.target\n",
    "X = data.data\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "red_x, red_y = [], []\n",
    "blue_x, blue_y = [], []\n",
    "green_x, green_y = [], []\n",
    "for i in range(len(reduced_X)):\n",
    "    if y[i] == 0:\n",
    "        red_x.append(reduced_X[i][0])\n",
    "        red_y.append(reduced_X[i][1])\n",
    "    elif y[i] == 1:\n",
    "        blue_x.append(reduced_X[i][0])\n",
    "        blue_y.append(reduced_X[i][1])\n",
    "    else:\n",
    "        green_x.append(reduced_X[i][0])\n",
    "        green_y.append(reduced_X[i][1])\n",
    "plt.scatter(red_x, red_y, c='r', marker='x')\n",
    "plt.scatter(blue_x, blue_y, c='b', marker='D')\n",
    "plt.scatter(green_x, green_y, c='g', marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly derived features are plotted in the above figure. Each of the dataset's three classes is indicated by its own marker style. From this two-dimensional view of the data, it is clear that one of the classes can be easily separated from the other two overlapping classes. It would be difficult to notice this structure without a graphical representation. This insight can inform our choice of classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
